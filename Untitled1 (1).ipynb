{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f13a230e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Samarth-\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Samarth-\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Samarth-\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a prompt: When the user clicks the Submit button\n",
      "Some words match. The content may be copied.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download NLTK resources (you only need to do this once)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "\n",
    "    # Join tokens into a single string\n",
    "    preprocessed_text = ' '.join(lemmatized_tokens)\n",
    "\n",
    "    return preprocessed_text\n",
    "\n",
    "# Function to check if any word matches\n",
    "def check_word_match(prompt, website_content):\n",
    "    # Tokenize and preprocess prompt and website content\n",
    "    prompt_tokens = word_tokenize(prompt.lower())\n",
    "    website_tokens = word_tokenize(website_content.lower())\n",
    "\n",
    "    prompt_filtered = [token for token in prompt_tokens if token not in stopwords.words('english')]\n",
    "    website_filtered = [token for token in website_tokens if token not in stopwords.words('english')]\n",
    "\n",
    "    preprocessed_prompt = [WordNetLemmatizer().lemmatize(token) for token in prompt_filtered]\n",
    "    preprocessed_website = [WordNetLemmatizer().lemmatize(token) for token in website_filtered]\n",
    "\n",
    "    # Check word similarity\n",
    "    for word in preprocessed_prompt:\n",
    "        if word in preprocessed_website:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# Get user input\n",
    "prompt = input(\"Enter a prompt: \")\n",
    "\n",
    "# Website content (replace with your web scraping code)\n",
    "website_content = \"With the vector embeddings added to the database and indexed, we’re ready to start finding similar content. When users submit their article text as input, a request is made to an API endpoint that uses Pinecone’s SDK to query the index of vector embeddings. The endpoint returns 10 similar articles that were possibly plagiarized and displays them in the app’s UI. That’s it! Simple enough, right?The UI features a simple textarea input in which the user can paste the text from an article. When the user clicks the Submit button, this input is used to query a database of articles. Results and their match scores are then displayed to the user. To help reduce the amount of noise, the app also includes a slider input in which the user can specify a similarity threshold to only show extremely strong matches.Plagiarism is rampant on the internet and in the classroom. With so much content out there, it’s sometimes hard to know when something has been plagiarized. Authors writing blog posts may want to check if someone has stolen their work and posted it elsewhere. Teachers may want to check students’ papers against other scholarly articles for copied work. News outlets may want to check if a content farm has stolen their news articles and claimed the content as its own.\"\n",
    "\n",
    "# Check if any word matches\n",
    "is_copied = check_word_match(prompt, website_content)\n",
    "\n",
    "# Output the result\n",
    "if is_copied:\n",
    "    print(\"Some words match. The content may be copied.\")\n",
    "else:\n",
    "    print(\"No matching words found. The content seems original.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc5ff72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
