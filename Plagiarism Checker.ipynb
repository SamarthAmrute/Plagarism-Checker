{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0907289c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Samarth-\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Samarth-\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Samarth-\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a prompt: When the user clicks the Submit button\n",
      "Some words match. The content may be copied.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download NLTK resources (you only need to do this once)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "\n",
    "    # Join tokens into a single string\n",
    "    preprocessed_text = ' '.join(lemmatized_tokens)\n",
    "\n",
    "    return preprocessed_text\n",
    "\n",
    "# Function to check if any word matches\n",
    "def check_word_match(prompt, website_content):\n",
    "    # Tokenize and preprocess prompt and website content\n",
    "    prompt_tokens = word_tokenize(prompt.lower())\n",
    "    website_tokens = word_tokenize(website_content.lower())\n",
    "\n",
    "    prompt_filtered = [token for token in prompt_tokens if token not in stopwords.words('english')]\n",
    "    website_filtered = [token for token in website_tokens if token not in stopwords.words('english')]\n",
    "\n",
    "    preprocessed_prompt = [WordNetLemmatizer().lemmatize(token) for token in prompt_filtered]\n",
    "    preprocessed_website = [WordNetLemmatizer().lemmatize(token) for token in website_filtered]\n",
    "\n",
    "    # Check word similarity\n",
    "    for word in preprocessed_prompt:\n",
    "        if word in preprocessed_website:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# Get user input\n",
    "prompt = input(\"Enter a prompt: \")\n",
    "\n",
    "# Website content (replace with your web scraping code)\n",
    "website_content = \"With the vector embeddings added to the database and indexed, we’re ready to start finding similar content. When users submit their article text as input, a request is made to an API endpoint that uses Pinecone’s SDK to query the index of vector embeddings. The endpoint returns 10 similar articles that were possibly plagiarized and displays them in the app’s UI. That’s it! Simple enough, right?The UI features a simple textarea input in which the user can paste the text from an article. When the user clicks the Submit button, this input is used to query a database of articles. Results and their match scores are then displayed to the user. To help reduce the amount of noise, the app also includes a slider input in which the user can specify a similarity threshold to only show extremely strong matches.Plagiarism is rampant on the internet and in the classroom. With so much content out there, it’s sometimes hard to know when something has been plagiarized. Authors writing blog posts may want to check if someone has stolen their work and posted it elsewhere. Teachers may want to check students’ papers against other scholarly articles for copied work. News outlets may want to check if a content farm has stolen their news articles and claimed the content as its own.\"\n",
    "\n",
    "# Check if any word matches\n",
    "is_copied = check_word_match(prompt, website_content)\n",
    "\n",
    "# Output the result\n",
    "if is_copied:\n",
    "    print(\"Some words match. The content may be copied.\")\n",
    "else:\n",
    "    print(\"No matching words found. The content seems original.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a43dfa",
   "metadata": {},
   "source": [
    "**Certainly! Let's go through the code and explain the theory behind it.**\n",
    "\n",
    "The code you provided is a Python script that demonstrates how to preprocess text and check for word matches between a user prompt and website content. Here's a breakdown of the important parts:\n",
    "\n",
    "**Importing necessary NLTK modules:**\n",
    "\n",
    "nltk is the Natural Language Toolkit library, which provides various tools and resources for working with human language data.\n",
    "stopwords from nltk.corpus contains a list of common words (e.g., \"the,\" \"is,\" \"and\") that are often removed from text as they don't contribute much to the overall meaning.\n",
    "word_tokenize from nltk.tokenize is used to split text into individual words or tokens.\n",
    "WordNetLemmatizer from nltk.stem is a tool for reducing words to their base or root form (e.g., \"running\" becomes \"run\").\n",
    "\n",
    "**Downloading NLTK resources:**\n",
    "\n",
    "The nltk.download function is used to download the necessary resources for tokenization, stopwords, and lemmatization. This step is required only once to ensure you have the required data.\n",
    "\n",
    "**Preprocessing Text:**\n",
    "\n",
    "The preprocess_text function takes a text input, tokenizes it into words, removes stop words, lemmatizes the remaining tokens, and then joins them back into a single string.\n",
    "Tokenization is the process of splitting text into individual words or tokens. word_tokenize is used to perform tokenization.\n",
    "Stop words are commonly occurring words that do not carry much information about the content of the text. The stopwords.words('english') returns a list of English stopwords, and the function removes these words from the tokenized text.\n",
    "Lemmatization reduces words to their base or root form. The WordNetLemmatizer is used to lemmatize the tokens.\n",
    "Finally, the preprocessed tokens are joined back into a single string using ' '.join(lemmatized_tokens).\n",
    "\n",
    "**Checking for Word Matches:**\n",
    "\n",
    "The check_word_match function takes a user prompt and website content as inputs and checks if any words in the prompt match with the words in the website content.\n",
    "The prompt and website content are tokenized and preprocessed using similar steps as in the preprocess_text function.\n",
    "The function then iterates through each word in the preprocessed prompt and checks if it exists in the preprocessed website content. If a match is found, the function returns True.\n",
    "If no match is found for any word in the prompt, the function returns False.\n",
    "\n",
    "**User Input and Website Content:**\n",
    "\n",
    "The code prompts the user to enter a prompt using the input function and stores it in the prompt variable.\n",
    "The website_content variable contains a sample text that represents the content of a website (e.g., obtained through web scraping).\n",
    "\n",
    "**Performing the Word Match Check:**\n",
    "\n",
    "The check_word_match function is called with the prompt and website_content as arguments to check if any words match.\n",
    "The result is stored in the is_copied variable.\n",
    "\n",
    "**Outputting the Result:**\n",
    "\n",
    "Finally, the code checks the value of is_copied and prints an appropriate message based on the result.\n",
    "The purpose of this code is to provide a simple demonstration of text preprocessing and word matching using NLTK. It can be used to check if a user prompt has any word matches with website content, which might suggest that the content has been copied or plagiarized."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
